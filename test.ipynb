{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import bitsandbytes as bnb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import LoraConfig, PeftConfig\n",
    "from trl import SFTTrainer\n",
    "from trl import setup_chat_format\n",
    "from transformers import (AutoModelForCausalLM, \n",
    "                          AutoTokenizer, \n",
    "                          BitsAndBytesConfig, \n",
    "                          TrainingArguments, \n",
    "                          pipeline, \n",
    "                          logging)\n",
    "from sklearn.metrics import (accuracy_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix,\n",
    "                             f1_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get working directory\n",
    "cwd = os.getcwd()\n",
    "data_dir = os.path.join(cwd, 'data')\n",
    "model_dir = os.path.join(cwd, 'model')\n",
    "\n",
    "# load data and pre-process datasets\n",
    "train_df = pd.read_csv(os.path.join(data_dir, 'gptTestNames.csv'))\n",
    "# test_df = pd.read_csv(os.path.join(data_dir, 'gptTestNames.csv'))\n",
    "# val_df = pd.read_csv(os.path.join(data_dir, 'gptValNames.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = list()\n",
    "# X_test = list()\n",
    "# for race in [\"API\", \"White\", \"Black\", \"Hispanic\"]:\n",
    "#     train, test  = train_test_split(train_df[train_df.label==race], \n",
    "#                                     train_size=300,\n",
    "#                                     test_size=300, \n",
    "#                                     random_state=42)\n",
    "#     X_train.append(train)\n",
    "#     X_test.append(test)\n",
    "\n",
    "X_train, X_test  = train_test_split(train_df, \n",
    "                                train_size=int(len(train_df) * 0.0008),\n",
    "                                test_size=int(len(train_df) * 0.0002), \n",
    "                                random_state=42)\n",
    "\n",
    "# X_train = pd.concat(X_train).sample(frac=1, random_state=10)\n",
    "# X_test = pd.concat(X_test)\n",
    "\n",
    "# eval_idx = [idx for idx in train_df.index if idx not in list(train.index) + list(test.index)]\n",
    "# X_eval = train_df[train_df.index.isin(eval_idx)]\n",
    "X_train, X_eval = train_test_split(X_train, \n",
    "                                test_size=0.1,\n",
    "                                random_state=42)\n",
    "X_train = X_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1867, 518, 208)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(data_point, shuffle=False):\n",
    "    if not shuffle:\n",
    "        return f\"\"\"\n",
    "                Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, Black, Hispanic, or White. \n",
    "                Your answer should only be the category name.\n",
    "                [{data_point[\"name\"]}].\n",
    "                ANSWER: {data_point[\"label\"]}\n",
    "                \"\"\".strip()\n",
    "    \n",
    "    categories = [\"Hispanic\", \"Black\", \"White\", \"Asian\"]\n",
    "    random.shuffle(categories)\n",
    "    categories_str = ', '.join(categories)\n",
    "    return f\"\"\"\n",
    "            Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: {categories_str}. \n",
    "            Your answer should only be the category name.\n",
    "            [{data_point[\"name\"]}]\n",
    "            ANSWER: {data_point[\"label\"]}\n",
    "            \"\"\".strip()\n",
    "\n",
    "def generate_test_prompt(data_point, shuffle=False):\n",
    "    if not shuffle:\n",
    "        return f\"\"\"\n",
    "                Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, Black, Hispanic, or White. \n",
    "                Your answer should only be the category name.\n",
    "                [{data_point[\"name\"]}]\n",
    "                ANSWER: \"\"\".strip()\n",
    "    \n",
    "    categories = [\"Hispanic\", \"Black\", \"White\", \"Asian\"]\n",
    "    random.shuffle(categories)\n",
    "    categories_str = ', '.join(categories)\n",
    "    return f\"\"\"\n",
    "            Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: {categories_str}. \n",
    "            Your answer should only be the category name.\n",
    "            [{data_point[\"name\"]}]\n",
    "            ANSWER: \"\"\".strip()\n",
    "\n",
    "\n",
    "X_train_1 = pd.DataFrame(X_train.apply(lambda row: generate_prompt(row, shuffle=True), axis=1), \n",
    "                       columns=[\"name\"])\n",
    "X_eval_1 = pd.DataFrame(X_eval.apply(lambda row: generate_prompt(row, shuffle=True), axis=1), \n",
    "                       columns=[\"name\"])\n",
    "\n",
    "y_true = X_test.label\n",
    "X_test_1 = pd.DataFrame(X_test.apply(lambda row: generate_test_prompt(row, shuffle=True), axis=1), \n",
    "                      columns=[\"name\"])\n",
    "X_test_2 = pd.DataFrame(X_test.apply(lambda row: generate_test_prompt(row, shuffle=False), axis=1), \n",
    "                      columns=[\"name\"])\n",
    "\n",
    "train_data = Dataset.from_pandas(X_train_1)\n",
    "eval_data = Dataset.from_pandas(X_eval_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true, y_pred):\n",
    "    labels = ['API', 'Black', 'Hispanic', 'White']\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n",
    "    print(f'Accuracy: {accuracy:.3f}')\n",
    "        \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true=y_true, y_pred=y_pred, target_names=labels)\n",
    "    print('\\nClassification Report:')\n",
    "    print(class_report)\n",
    "    \n",
    "    # Generate confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=labels)\n",
    "    print('\\nConfusion Matrix:')\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d087d27eae471e9ab86d23da342436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=bnb_config, \n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, \n",
    "                                          trust_remote_code=True,\n",
    "                                         )\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test, model, tokenizer):\n",
    "    y_pred = []\n",
    "    for i in tqdm(range(len(test))):\n",
    "    # for i in [69, 222, 676, 1270, 2060, 3684, 3827, 4472, 4799, 4972, 5120]:\n",
    "        prompt = test.iloc[i][\"name\"]\n",
    "        pipe = pipeline(task=\"text-generation\", \n",
    "                        model=model, \n",
    "                        tokenizer=tokenizer, \n",
    "                        max_new_tokens = 4, \n",
    "                        # temperature = 0.01,\n",
    "                        do_sample = False,\n",
    "                       )\n",
    "        result = pipe(prompt)\n",
    "        answer = result[0]['generated_text'].split(\":\")[-1].lower()\n",
    "        # print(prompt, answer)\n",
    "        if \"asian\" in answer:\n",
    "            y_pred.append(\"API\")\n",
    "        elif \"black\" in answer:\n",
    "            y_pred.append(\"Black\")\n",
    "        elif \"hispanic\" in answer:\n",
    "            y_pred.append(\"Hispanic\")\n",
    "        elif \"white\" in answer:\n",
    "            y_pred.append(\"White\")\n",
    "        else:\n",
    "            y_pred.append(\"none\")\n",
    "            print(prompt,answer)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/518 [00:00<?, ?it/s]/WAVE/projects/newsq_scu/xiaoxiao_git/Llama-race/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/WAVE/projects/newsq_scu/xiaoxiao_git/Llama-race/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 518/518 [07:09<00:00,  1.21it/s]\n",
      "100%|██████████| 518/518 [07:10<00:00,  1.20it/s]\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict(X_test_1, model, tokenizer)\n",
    "y_pred1 = predict(X_test_2, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_res = set(y_pred)  # Get unique labels\n",
    "print(unique_res)\n",
    "\n",
    "unique_labels = set(y_true)\n",
    "print(unique_labels)\n",
    "\n",
    "labels = list(set(list(set(y_true))+list(set(y_pred))))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.463\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         API       0.07      0.55      0.13        11\n",
      "       Black       0.20      0.46      0.28        80\n",
      "    Hispanic       0.75      0.73      0.74        92\n",
      "       White       0.80      0.39      0.52       335\n",
      "\n",
      "    accuracy                           0.46       518\n",
      "   macro avg       0.46      0.53      0.42       518\n",
      "weighted avg       0.68      0.46      0.52       518\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  6   4   1   0]\n",
      " [ 14  37   5  24]\n",
      " [  4  13  67   8]\n",
      " [ 56 133  16 130]]\n",
      "Accuracy: 0.320\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         API       0.04      1.00      0.07        11\n",
      "       Black       0.37      0.31      0.34        80\n",
      "    Hispanic       0.93      0.72      0.81        92\n",
      "       White       0.88      0.19      0.31       335\n",
      "\n",
      "    accuracy                           0.32       518\n",
      "   macro avg       0.55      0.56      0.38       518\n",
      "weighted avg       0.79      0.32      0.40       518\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 11   0   0   0]\n",
      " [ 45  25   2   8]\n",
      " [ 25   0  66   1]\n",
      " [226  42   3  64]]\n"
     ]
    }
   ],
   "source": [
    "# y_pred = ['API' if 'Asia Pacific Islander' in x else x for x in y_pred]\n",
    "evaluate(y_true, y_pred)\n",
    "evaluate(y_true, y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Micro): 0.463\n",
      "F1 Score (Macro): 0.418\n",
      "F1 Score (Weighted): 0.515\n",
      "--------------below is for shuffle test----------------\n",
      "F1 Score (Micro): 0.320\n",
      "F1 Score (Macro): 0.383\n",
      "F1 Score (Weighted): 0.401\n"
     ]
    }
   ],
   "source": [
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 Score (Micro): {f1_micro:.3f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.3f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.3f}\")\n",
    "\n",
    "print(\"--------------below is for shuffle test----------------\")\n",
    "f1_micro = f1_score(y_true, y_pred1, average='micro')\n",
    "f1_macro = f1_score(y_true, y_pred1, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred1, average='weighted')\n",
    "print(f\"F1 Score (Micro): {f1_micro:.3f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.3f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Hispanic, Asian, White, Black. \n",
      "            Your answer should only be the category name.\n",
      "            [Saucedo Cesar]\n",
      "            ANSWER:\n",
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, Black, Hispanic, or White. \n",
      "                Your answer should only be the category name.\n",
      "                [Saucedo Cesar]\n",
      "                ANSWER:\n",
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: White, Asian, Hispanic, Black. \n",
      "            Your answer should only be the category name.\n",
      "            [Tarnecki Lori]\n",
      "            ANSWER: White\n"
     ]
    }
   ],
   "source": [
    "print(X_test_1.iloc[0][\"name\"])\n",
    "print(X_test_2.iloc[0][\"name\"])\n",
    "print(X_train_1.iloc[0][\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/WAVE/projects/newsq_scu/xiaoxiao_git/Llama-race/venv/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:223: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c80072284a78437795c8e7b96c8ce915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1867 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e7f3e4070942c090f9633426fac871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/208 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "output_dir=\"trained_weigths\"\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=16, \n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,                    # directory to save and repository id\n",
    "    num_train_epochs=2,                       # number of training epochs\n",
    "    per_device_train_batch_size=4,            # batch size per device during training\n",
    "    gradient_accumulation_steps=1,            # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,              # use gradient checkpointing to save memory\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=0,\n",
    "    logging_steps=25,                         # log every 10 steps\n",
    "    learning_rate=2e-4,                       # learning rate, based on QLoRA paper\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,                        # max gradient norm based on QLoRA paper\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,                        # warmup ratio based on QLoRA paper\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",               # use cosine learning rate scheduler\n",
    "    report_to=\"tensorboard\",                  # report metrics to tensorboard\n",
    "    evaluation_strategy=\"epoch\"               # save checkpoint every epoch\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"name\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_length=None,\n",
    "    packing=False,\n",
    "    dataset_batch_size=4,\n",
    "    # dataset_kwargs={\n",
    "    #     \"add_special_tokens\": False,\n",
    "    #     \"append_concat_token\": False,\n",
    "    # }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/WAVE/projects/newsq_scu/xiaoxiao_git/Llama-race/venv/lib/python3.11/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='467' max='467' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [467/467 32:28, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354500</td>\n",
       "      <td>0.387827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=467, training_loss=0.504207287406513, metrics={'train_runtime': 1953.4371, 'train_samples_per_second': 0.956, 'train_steps_per_second': 0.239, 'total_flos': 4618404962795520.0, 'train_loss': 0.504207287406513, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(output_dir)\n",
    "trainer.tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/518 [00:00<?, ?it/s]/WAVE/projects/newsq_scu/xiaoxiao_git/Llama-race/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/WAVE/projects/newsq_scu/xiaoxiao_git/Llama-race/venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "  7%|▋         | 38/518 [00:29<06:13,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, Hispanic, White, Black. \n",
      "            Your answer should only be the category name.\n",
      "            [Vongrasamy Phouphet]\n",
      "            ANSWER:  apia\n",
      "           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 46/518 [00:35<06:14,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, White, Hispanic, Black. \n",
      "            Your answer should only be the category name.\n",
      "            [Nguyen Nhuy]\n",
      "            ANSWER:  apia\n",
      "           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 348/518 [04:30<02:11,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Hispanic, Asian, Black, White. \n",
      "            Your answer should only be the category name.\n",
      "            [Lu Libo]\n",
      "            ANSWER:  apia\n",
      "           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 405/518 [05:14<01:27,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, White, Black, Hispanic. \n",
      "            Your answer should only be the category name.\n",
      "            [Tufts Mikyong]\n",
      "            ANSWER:  apia\n",
      "           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518/518 [06:42<00:00,  1.29it/s]\n",
      "  7%|▋         | 38/518 [00:29<06:12,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, Black, Hispanic, or White. \n",
      "                Your answer should only be the category name.\n",
      "                [Vongrasamy Phouphet]\n",
      "                ANSWER:  apia\n",
      "           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 46/518 [00:35<06:13,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, Black, Hispanic, or White. \n",
      "                Your answer should only be the category name.\n",
      "                [Nguyen Nhuy]\n",
      "                ANSWER:  api\n",
      "            }\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 348/518 [04:30<02:11,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, Black, Hispanic, or White. \n",
      "                Your answer should only be the category name.\n",
      "                [Lu Libo]\n",
      "                ANSWER:  apia\n",
      "           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 405/518 [05:14<01:27,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, Black, Hispanic, or White. \n",
      "                Your answer should only be the category name.\n",
      "                [Tufts Mikyong]\n",
      "                ANSWER:  apia\n",
      "           \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 461/518 [05:58<00:44,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guess the race of the name enclosed in square brackets into 1 of the following 4 categories: Asian, Black, Hispanic, or White. \n",
      "                Your answer should only be the category name.\n",
      "                [Nagassar Sharmila]\n",
      "                ANSWER:  api\n",
      "            </\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518/518 [06:42<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "y_pred = predict(X_test_1, model, tokenizer)\n",
    "y_pred1 = predict(X_test_2, model, tokenizer)\n",
    "# evaluate(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.811\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         API       1.00      0.45      0.62        11\n",
      "       Black       0.67      0.17      0.28        80\n",
      "    Hispanic       0.90      0.84      0.87        92\n",
      "       White       0.80      0.97      0.87       335\n",
      "\n",
      "    accuracy                           0.81       518\n",
      "   macro avg       0.84      0.61      0.66       518\n",
      "weighted avg       0.80      0.81      0.78       518\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5   0   1   5]\n",
      " [  0  14   4  62]\n",
      " [  0   0  77  15]\n",
      " [  0   7   4 324]]\n",
      "Accuracy: 0.817\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         API       1.00      0.55      0.71        11\n",
      "       Black       0.59      0.21      0.31        80\n",
      "    Hispanic       0.90      0.86      0.88        92\n",
      "       White       0.81      0.96      0.88       335\n",
      "\n",
      "    accuracy                           0.82       518\n",
      "   macro avg       0.82      0.64      0.69       518\n",
      "weighted avg       0.80      0.82      0.79       518\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  6   0   1   4]\n",
      " [  0  17   5  58]\n",
      " [  0   1  79  12]\n",
      " [  0  11   3 321]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = ['API' if 'none' in x else x for x in y_pred]\n",
    "y_pred1 = ['API' if 'none' in x else x for x in y_pred1]\n",
    "evaluate(y_true, y_pred)\n",
    "evaluate(y_true, y_pred1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score (Micro): 0.811\n",
      "F1 Score (Macro): 0.660\n",
      "F1 Score (Weighted): 0.775\n",
      "--------------below is for shuffle test----------------\n",
      "F1 Score (Micro): 0.817\n",
      "F1 Score (Macro): 0.694\n",
      "F1 Score (Weighted): 0.788\n"
     ]
    }
   ],
   "source": [
    "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
    "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred, average='weighted')\n",
    "print(f\"F1 Score (Micro): {f1_micro:.3f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.3f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.3f}\")\n",
    "print(\"--------------below is for shuffle test----------------\")\n",
    "f1_micro = f1_score(y_true, y_pred1, average='micro')\n",
    "f1_macro = f1_score(y_true, y_pred1, average='macro')\n",
    "f1_weighted = f1_score(y_true, y_pred1, average='weighted')\n",
    "print(f\"F1 Score (Micro): {f1_micro:.3f}\")\n",
    "print(f\"F1 Score (Macro): {f1_macro:.3f}\")\n",
    "print(f\"F1 Score (Weighted): {f1_weighted:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=['API','Black','Hispanic','White'])\n",
    "metrics_per_class = {}\n",
    "tpr, tnr = [], []\n",
    "for i in range(len(cm)):\n",
    "    TP = cm[i, i]\n",
    "    FP = sum(cm[:, i]) - TP\n",
    "    FN = sum(cm[i, :]) - TP\n",
    "    TN = sum(cm.sum(axis=1)) - TP - FP - FN\n",
    "    TPR = TP / float(TP + FN) if (TP + FN) > 0 else 0\n",
    "    TNR = TN / float(TN + FP) if (TN + FP) > 0 else 0\n",
    "    # class_label = le.inverse_transform([i])[0]  # Convert index back to original class label\n",
    "    # metrics_per_class[class_label] = {'TPR': TPR, 'TNR': TNR}\n",
    "    tpr.append(TPR)\n",
    "    tnr.append(TNR)\n",
    "\n",
    "temp = (np.array(tpr)+np.array(tnr))*0.5\n",
    "# print(temp)\n",
    "gap = 0.0\n",
    "for i in range(len(temp)-1):\n",
    "    gap += abs(temp[i]-temp[-1])\n",
    "gap /= 3\n",
    "print('1-GAP is:',1-gap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recalls = [0.09, 0.97, 0.4, 0.02]\n",
    "\n",
    "dis = 0.0\n",
    "for i in range(len(recalls)-1):\n",
    "    dis += recalls[i]/recalls[-1]\n",
    "dis /= (len(recalls)-1)\n",
    "print('Disparate Impact is:',dis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
